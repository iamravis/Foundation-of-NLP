{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Corpora, Tokens, and Types\n",
    "\n",
    "\n",
    "1. `Corpus :` A corpus usually contains raw text (in ASCII or UTF-8) and any metadata associated with the text.\n",
    "\n",
    "2.  `Tokens :` correspond to words and numeric sequences separated by white-space characters or punctuation.\n",
    "\n",
    "3.  `Instance :` In machine learning parlance, the text along with its metadata is called an instance or data point. \n",
    "\n",
    "![Alt text](images/nlpp_0201.png)\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Tokenisation\n",
    "\n",
    "The process of breaking a text down into tokens is called `tokenization`.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sruthi', ',', 'do', \"n't\", 'slap', 'the', 'witch']\n"
     ]
    }
   ],
   "source": [
    "# make sure to install spacy and load the model \n",
    "\n",
    "''' \n",
    "Use following command to install spacy using conda enviroment \n",
    "\n",
    "1. conda install -c conda-forge spacy  \n",
    "2. python -m spacy download en_core_web_sm\n",
    "\n",
    "'''\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"Sruthi, don't slap the witch\"\n",
    "\n",
    "print([str(token) for token in nlp(text)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use NLTK library and perform the tokenisation. See the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens : ['Hello', 'Mister', 'how', 'do', 'you', 'do', '?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet = \"Hello Mister how do you do?\"\n",
    "\n",
    "tokeniser = TweetTokenizer()\n",
    "\n",
    "print(f\" Tokens : {tokeniser.tokenize(tweet)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Notes\n",
    "\n",
    "1. `Types` are unique tokens present in a corpus. \n",
    "\n",
    "2. The set of all types in a corpus is its `vocabulary` or `lexicon`. \n",
    "\n",
    "3. Words can be distinguished as `content words` and `stopwords`. \n",
    "\n",
    "4. Stopwords such as articles and prepositions serve mostly a grammatical purpose, like filler holding the content words.\n",
    "\n",
    "\n",
    "- This process of understanding the linguistics of a language and applying it to solving NLP problems is called `feature engineering`. \n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams, Bigrams, Trigrams, …, N-grams\n",
    "\n",
    "- N-grams are fixed-length (n) consecutive token sequences occurring in the text. A `bigram` has two tokens, a `unigram` one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens in the sentence : ['Real', 'madrid', 'is', 'the', 'greatest', 'football', 'club', 'in', 'the', 'history', 'of', 'football']\n",
      "\n",
      " n-grams of length 2 (bigrams): [['Real', 'madrid'], ['madrid', 'is'], ['is', 'the'], ['the', 'greatest'], ['greatest', 'football'], ['football', 'club'], ['club', 'in'], ['in', 'the'], ['the', 'history'], ['history', 'of'], ['of', 'football']]\n"
     ]
    }
   ],
   "source": [
    "def n_grams(text, n):\n",
    "\n",
    "   '''\n",
    "   Takes text and number and return a list of n-grams\n",
    "   '''\n",
    "\n",
    "   return [text[i:i+n] for i in range(len(text) - n+1)]\n",
    "\n",
    "## Lets start with a sentence \n",
    "sentence  = \"Real madrid is the greatest football club in the history of football\"\n",
    "\n",
    "## Now lets use spacy model to collect the tokens in this sentence\n",
    "## Make sure you have loadded the spacy model ## nlp = spacy.load('en_core_web_sm')\n",
    "tokens = [str(tokens) for tokens in nlp(sentence)]\n",
    "\n",
    "print(f\" Tokens in the sentence : {tokens}\\n\")\n",
    "\n",
    "## Now that we have collected the tokens we can extract n grams from the token list\n",
    "\n",
    "print(f\" n-grams of length 2 (bigrams): {n_grams(tokens, 2)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Lemmas and Stems\n",
    "\n",
    "1. `Lemmas` are root forms of words. Consider the verb fly. It can be inflected into many different words—`flow, flew, flies, flown, flowing,` and so on—and fly is the lemma for all of these seemingly different words.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ---> he\n",
      "was ---> be\n",
      "running ---> run\n",
      "late ---> late\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "## Lets start with a sentence \n",
    "sentence  = \"Real madrid is the greatest football club in the history of football\"\n",
    "\n",
    "## we can also create a simple lemmetizer function which takes and sentence and gives us the lemma\n",
    "\n",
    "def show_lemma(sentence):\n",
    "    ''' \n",
    "    Takes normal sentence and return lemma\n",
    "    '''\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    for tokens in doc:\n",
    "        print('{} ---> {}'.format(tokens, tokens.lemma_))\n",
    "\n",
    "\n",
    "show_lemma(\"he was running late\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "- `spaCy`, for example, uses a predefined dictionary, called `WordNet`, for extracting lemmas, but `lemmatization` can be framed as a machine learning problem requiring an understanding of the `morphology of the language`.\n",
    "\n",
    "\n",
    "### NOTE : We are not going to talk about `stemming` here \n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real - ADJ\n",
      "madrid - PROPN\n",
      "is - AUX\n",
      "the - DET\n",
      "greatest - ADJ\n",
      "football - NOUN\n",
      "club - NOUN\n",
      "in - ADP\n",
      "the - DET\n",
      "history - NOUN\n",
      "of - ADP\n",
      "football - NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Often, we need to label a `span of text`; that is, a contiguous `multitoken` boundary. For example, consider the sentence, “Mary slapped the green witch.” We might want to identify the `noun phrases (NP)` and `verb phrases (VP)` in it, as shown here:\n",
    "\n",
    "[NP Mary] [VP slapped] [the green witch].\n",
    "\n",
    "\n",
    "This is called `chunking` or `shallow parsing`. \n",
    "\n",
    "\n",
    "`Shallow parsing` aims to derive higher-order units composed of the grammatical atoms, like nouns, verbs, adjectives, and so on. It is possible to write regular expressions over the part-of-speech tags to approximate shallow parsing if you do not have data to train models for shallow parsing. \n",
    "\n",
    "![Alt text](images/nlpp_0203.png)\n",
    "\n",
    "\n",
    "Parse trees indicate how different grammatical units in a sentence are related hierarchically. The parse tree in this figure shows what’s called a constituent parse.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - NP\n",
      "the green witch - NP\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc  = nlp(u\"Mary slapped the green witch.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print ('{} - {}'.format(chunk, chunk.label_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Word Senses and Semantics\n",
    "\n",
    "Words have meanings, and often more than one. \n",
    "\n",
    "The different meanings of a word are called its `senses`. \n",
    "\n",
    "`WordNet`, a long-running lexical resource project from Princeton University, aims to catalog the senses of all (well, most) words in the English language, along with other lexical relationships.4 For example, consider a word like “plane.” shows the different senses in which this word could be used.\n",
    "\n",
    "![Alt text](images/nlpp_0205.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:13) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a00163766e90d89a5652dcf8fc93c290a5a79be566c67c844c6a6d213af21088"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
